{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstatic_image_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodel_complexity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_tracking_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "MediaPipe Hands.\n",
      "\n",
      "MediaPipe Hands processes an RGB image and returns the hand landmarks and\n",
      "handedness (left v.s. right hand) of each detected hand.\n",
      "\n",
      "Note that it determines handedness assuming the input image is mirrored,\n",
      "i.e., taken with a front-facing/selfie camera (\n",
      "https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n",
      "horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n",
      "to flip the image first for a correct handedness output.\n",
      "\n",
      "Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n",
      "usage examples.\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Initializes a MediaPipe Hand object.\n",
      "\n",
      "Args:\n",
      "  static_image_mode: Whether to treat the input images as a batch of static\n",
      "    and possibly unrelated images, or a video stream. See details in\n",
      "    https://solutions.mediapipe.dev/hands#static_image_mode.\n",
      "  max_num_hands: Maximum number of hands to detect. See details in\n",
      "    https://solutions.mediapipe.dev/hands#max_num_hands.\n",
      "  model_complexity: Complexity of the hand landmark model: 0 or 1.\n",
      "    Landmark accuracy as well as inference latency generally go up with the\n",
      "    model complexity. See details in\n",
      "    https://solutions.mediapipe.dev/hands#model_complexity.\n",
      "  min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n",
      "    detection to be considered successful. See details in\n",
      "    https://solutions.mediapipe.dev/hands#min_detection_confidence.\n",
      "  min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
      "    hand landmarks to be considered tracked successfully. See details in\n",
      "    https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\shory\\appdata\\roaming\\python\\python39\\site-packages\\mediapipe\\python\\solutions\\hands.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "mp.solutions.hands.Hands?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "pTime = 0\n",
    "cTime = 0\n",
    "\n",
    "while True:\n",
    "  success, img = cap.read()\n",
    "  imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "  results = hands.process(imgRGB)\n",
    "  # print(results.multi_hand_landmarks)\n",
    "  \n",
    "  if results.multi_hand_landmarks:\n",
    "    for handlms in results.multi_hand_landmarks:\n",
    "      for id, lm in enumerate(handlms.landmark):\n",
    "        # print(id, lm) #this prints the ratio\n",
    "        h, w, c = img.shape\n",
    "        cx, cy = int(lm.x * w) , int(lm.y * h) \n",
    "        # print(id , cx, cy) #this prints the pixel values of x and y coordinate of an given point on hand\n",
    "        \n",
    "        \n",
    "        if id == 5:\n",
    "          cv2.circle(img, (cx,cy),10,(0,255,0),cv2.FILLED)\n",
    "        \n",
    "          \n",
    "      mpDraw.draw_landmarks(img,handlms, mpHands.HAND_CONNECTIONS)\n",
    "  \n",
    "  cTime = time.time()\n",
    "  \n",
    "  fps = 1/(cTime-pTime)\n",
    "  \n",
    "  pTime = cTime\n",
    "  \n",
    "  cv2.putText(img, str(int(fps)), (10,70), cv2.FONT_HERSHEY_PLAIN, 3 , (0,255,255), 3)\n",
    "  \n",
    "  cv2.imshow(\"Image\",img)\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
